# 机器学习与深度学习知识点提纲

---

## 📌 模型性能与误差分析

### 偏差-方差分解
- **核心定义与目的**：描述模型泛化误差来源，分为偏差、方差、不可约误差。
- **关键原理**：总误差 = 偏差² + 方差 + 噪声。
- **常见误区**：偏差和方差可以同时很小；增加数据能解决所有问题。
- **举例说明**：低阶模型偏差大，高阶模型方差大。
- **相关对比**：欠拟合（偏差主导） vs 过拟合（方差主导）。

### 精确率、召回率、FPR、混淆矩阵
- **核心定义与目的**：评估分类模型的不同维度表现。
- **关键原理**：
  - 精确率 = TP / (TP + FP)
  - 召回率 = TP / (TP + FN)
  - FPR = FP / (FP + TN)
- **常见误区**：召回率高 ≠ 精确率高。
- **举例说明**：正负样本极度不平衡时，1% 的 FPR 也可能让精确率掉到 50%。

### Naive Bayes 与特征问题
- **高频词**：对分类无帮助，会降低性能。
- **重复特征**：人为放大权重，精度下降。
- **高度相关特征**：破坏独立性假设，但不同于重复特征。

---

## 📌 算法与模型

### K-means 聚类与分类的区别
- **核心定义**：K-means 是无监督聚类，不能直接做分类。
- **关键原理**：基于欧式距离最小化簇内方差。
- **常见误区**：K-means 聚类结果等同于类别。
- **相关对比**：K-means vs SVM/朴素贝叶斯（监督分类）。

### KNN（分类、回归、归一化敏感性）
- **核心定义**：基于“相似样本输出相似”的实例学习。
- **分类任务**：多数投票。
- **回归任务**：邻居均值。
- **归一化敏感性**：不同量纲特征会导致大尺度特征支配距离。
- **相关对比**：标准化 vs 归一化。

### 决策树与 CART
- **核心定义**：基于特征划分的树模型。
- **分类任务**：ID3、C4.5、CART 都能做。
- **回归任务**：只有 CART 可以做（平方误差最小化）。
- **常见误区**：所有决策树都能做回归。

### 朴素贝叶斯分类器（文本分类）
- **核心定义**：基于贝叶斯定理，假设特征独立。
- **关键原理**：
  $ P(y|d) \propto P(y) \prod_{w \in d} P(w|y) $
- **常见误区**：NB 能自动忽略无用特征。
- **改进**：停用词、TF-IDF、特征选择。

---

## 📌 深度学习模型与参数计算

### MLP 参数计算（Keras）
- **公式**：参数数 = 输入维度 × 输出单元数 + 输出单元数。
- **偏置大小**：= 输出单元数。
- **举例**：输入 100 → Dense(32) → Dense(1)，总参数 = 3265。
- **常见误区**：Dropout 层有参数（实际上没有）。

---

## 📌 优化与学习问题

### 黑盒攻击（对抗样本）
- **核心定义**：只知道输入输出，不知道结构与权重。
- **方法**：
  - 梯度估计（有限差分）
  - 优化/进化搜索（遗传算法）
- **常见误区**：黑盒攻击必须知道概率分布；只要隐藏结构就能防御。
- **相关对比**：白盒攻击（利用真实梯度） vs 黑盒攻击。

### 特征缩放对模型的影响
- **KNN、SVM**：强依赖距离度量 → 必须归一化/标准化。
- **树模型**：不敏感。

---

## 📌 经典理论与数学基础

### 线性规划对偶性
- **核心定义**：每个线性规划问题都有对应对偶问题。
- **关键原理**：弱对偶、强对偶定理。
- **解的关系**：
  - 原问题无界 → 对偶不可行
  - 对偶无界 → 原问题不可行
  - 原、对偶都可行 → 强对偶成立，最优值相等
- **常见误区**：原问题无解，对偶一定无解。
- **举例说明**：最小化 x s.t. x≥1 vs 对偶最大化问题。

---
