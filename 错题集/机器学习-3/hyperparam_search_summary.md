# 超参数搜索 (Grid Search vs Random Search) 知识点提纲

## 1. 基本概念
- **超参数 (Hyperparameter)**：训练前设定的参数（如学习率、正则化系数、树深度等）。
- **超参数搜索**：在给定搜索空间内寻找最优超参数组合，以提升模型性能。

## 2. Grid Search (网格搜索)
- **原理**：对每个超参数划定范围并等间隔取值，枚举所有组合，逐一训练模型并验证。
- **优点**：
  - 简单、系统化。
  - 在低维空间中能保证覆盖所有候选组合。
- **缺点**：
  - 维度灾难：参数多时组合数爆炸。
  - 在不重要的维度上浪费计算资源。

## 3. Random Search (随机搜索)
- **原理**：在搜索空间中随机采样一部分组合进行评估。
- **优点**：
  - 在高维空间中更高效。
  - 更快找到对模型性能敏感的超参数。
  - 可灵活扩展，逐步增加采样数。
- **缺点**：
  - 结果具有随机性，需设定采样次数。
  - 对搜索分布依赖较大（如学习率常用 log-uniform）。

## 4. 对比总结
| 方法          | 覆盖范围 | 高维效率 | 是否系统性 | 代表特点 |
|---------------|----------|----------|------------|----------|
| Grid Search   | 全面枚举 | 很低     | 强         | 适合低维、小范围搜索 |
| Random Search | 随机覆盖 | 较高     | 弱         | 适合高维、大范围搜索 |

## 5. 常见改进方法
- **贝叶斯优化 (Bayesian Optimization)**：利用先验和观测结果建立代理模型，指导下次搜索位置。
- **进化算法 (Genetic Algorithms)**：模拟进化过程优化超参数。
- **Hyperband/Successive Halving**：在有限计算资源下高效筛选超参数。

## 6. 实践建议
- 低维、参数范围小 → Grid Search。
- 高维、连续参数范围大 → Random Search。
- 有足够预算且需更高效 → 贝叶斯优化或 Hyperband。
