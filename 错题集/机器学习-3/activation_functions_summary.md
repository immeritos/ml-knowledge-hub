# 激活函数知识点提纲（ReLU vs Sigmoid/Tanh）

## 1. 基本概念
- **激活函数**：引入非线性，使神经网络能够拟合复杂函数关系。

## 2. Sigmoid 函数
- **定义**：
  ```math 
  \sigma(x) = \frac{1}{1 + e^{-x}}
  ```
- **特点**：
  - 输出范围 $(0, 1)$，可解释为概率。
  - 单调递增，平滑。
- **缺点**：
  - 容易导致 **梯度消失**（在饱和区梯度接近 0）。
  - 输出非零均值，收敛速度慢。

## 3. Tanh 函数
- **定义**：
  ```math 
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  ```
- **特点**：
  - 输出范围 $(-1, 1)$，零中心化。
  - 相比 Sigmoid 收敛更快。
- **缺点**：
  - 仍存在 **梯度消失问题**。

## 4. ReLU 函数
- **定义**：
  ```math
  f(x) = \max(0, x)
  ```
- **特点**：
  - 计算简单，收敛快。
  - 有效缓解梯度消失问题。
  - 稀疏激活，提高模型表达能力。
- **缺点**：
  - **ReLU 死亡问题**：当输入 < 0 时，梯度为 0，神经元可能永久失活。

## 5. 变种 ReLU
- **Leaky ReLU**：在负区间保留一个较小斜率，避免神经元死亡。
- **ELU/SELU**：在负区间使用平滑曲线，进一步改善梯度流动。

## 6. 对比总结
| 激活函数  | 输出范围   | 零中心化 | 梯度消失 | 优点              | 缺点               |
|-----------|------------|----------|----------|-------------------|--------------------|
| Sigmoid   | $(0, 1)$     | 否       | 严重     | 可解释为概率      | 梯度消失，非零均值 |
| Tanh      | $(-1, 1)$    | 是       | 较严重   | 收敛快，零均值    | 梯度消失           |
| ReLU      | $[0, \infty)$     | 否       | 轻微     | 快速收敛，稀疏性  | 神经元死亡         |

## 7. 实践建议
- 隐藏层常用 ReLU 或其变种。
- 输出层：
  - 二分类 → Sigmoid。
  - 多分类 → Softmax。
  - 回归 → 线性激活。

