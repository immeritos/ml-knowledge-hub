# 集成学习知识点提纲

## 1. 基本概念
- **集成学习 (Ensemble Learning)**：通过结合多个基学习器的预测结果来提升模型性能。
- **核心思想**：多个弱学习器结合，形成一个强学习器。

## 2. 主要方法类别
1. **Bagging (Bootstrap Aggregating)**
   - **原理**：对训练集进行有放回采样，训练多个基学习器，最终结果通过投票或平均。
   - **代表算法**：随机森林 (Random Forest)。
   - **特点**：
     - 降低方差，提升稳定性。
     - 各学习器可并行训练。
   - **适用场景**：高方差模型（如决策树）。

2. **Boosting**
   - **原理**：串行训练多个弱学习器，每一轮关注上轮错误样本，调整样本权重。
   - **代表算法**：AdaBoost, Gradient Boosting, XGBoost, LightGBM。
   - **特点**：
     - 降低偏差，提高准确率。
     - 学习器间存在依赖关系，难以并行。
   - **适用场景**：偏差较大的弱模型。

3. **Stacking**
   - **原理**：训练多个基学习器，将它们的预测结果作为新的特征，再训练一个次级学习器（meta-learner）。
   - **特点**：
     - 能够融合不同类型模型的优势。
     - 更复杂，需防止过拟合。
   - **适用场景**：模型多样性高的情况。

4. **Blending**
   - **原理**：类似 Stacking，但使用验证集来训练次级学习器，避免交叉验证。
   - **特点**：
     - 实现简单，训练速度快。
     - 利用验证集信息，可能浪费部分数据。
   - **适用场景**：快速模型融合。

## 3. 对比总结
| 方法      | 训练方式 | 并行性 | 偏差/方差影响 | 代表算法 |
|-----------|----------|--------|----------------|----------|
| Bagging   | 并行     | 高     | 降低方差       | 随机森林 |
| Boosting  | 串行     | 低     | 降低偏差       | AdaBoost, XGBoost |
| Stacking  | 多模型+次级学习器 | 中 | 综合偏差和方差 | 多层模型融合 |
| Blending  | 类 Stacking, 用验证集 | 中 | 综合偏差和方差 | Kaggle 模型融合常用 |

## 4. 常见问题
- Bagging/Boosting 主要关注同质模型（多数是决策树），Stacking/Blending 更适合异质模型。
- Boosting 容易过拟合，需要正则化（如学习率 shrinkage、子采样）。
- Stacking 需注意数据划分与交叉验证，避免信息泄漏。

## 5. 应用场景
- **Bagging**：适合高方差模型（如决策树），用于稳定性要求高的场景。
- **Boosting**：适合需要高精度的分类/回归任务（竞赛常用）。
- **Stacking/Blending**：适合模型融合，尤其在 Kaggle 比赛中广泛应用。
