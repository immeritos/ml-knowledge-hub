# 优化算法知识点提纲（AdaGrad, RMSProp）

## 1. 基本概念
- **优化算法**：用于更新神经网络参数，使损失函数最小化。
- **改进动机**：传统 SGD 存在学习率固定、收敛速度慢、难以适应不同参数稀疏性的问题。

## 2. AdaGrad
- **核心思想**：为不同参数分配不同的自适应学习率，常见于稀疏数据场景。
- **更新规则**：
  \[ g_t = \nabla_\theta L(\theta_t) \]
  \[ G_t = G_{t-1} + g_t^2 \]
  \[ \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} g_t \]
- **特点**：
  - 适合处理稀疏特征（如 NLP 中的词嵌入）。
  - 学习率单调递减，训练后期过早趋近于 0，可能停止学习。

## 3. RMSProp
- **核心思想**：改进 AdaGrad，使用指数加权移动平均 (EMA) 来控制历史梯度累积。
- **更新规则**：
  \[ g_t = \nabla_\theta L(\theta_t) \]
  \[ E[g^2]_t = \rho E[g^2]_{t-1} + (1-\rho) g_t^2 \]
  \[ \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t \]
- **特点**：
  - 缓解了 AdaGrad 学习率过快衰减问题。
  - 在非凸优化（如深度学习）中表现更好。
  - 引入超参数 \(\rho\)（一般取 0.9）。

## 4. 对比总结
| 算法    | 学习率调整方式 | 优点                   | 缺点                  | 应用场景 |
|---------|----------------|------------------------|-----------------------|----------|
| AdaGrad | 梯度平方累积   | 稀疏特征有效，自动调节 | 学习率衰减过快        | NLP, 稀疏数据 |
| RMSProp | 移动平均       | 收敛快，适合非凸问题   | 需要调节超参数 ρ      | 深度学习常用 |

## 5. 与其他优化器的关系
- **Adam**：结合 RMSProp 的自适应学习率思想 + 动量方法。

## 6. 实践建议
- **AdaGrad**：适合稀疏特征数据，若训练较长需谨慎。
- **RMSProp**：深度学习中更常用，推荐作为默认选择之一。
