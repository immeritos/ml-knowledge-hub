# Dropout 知识点提纲

## 1. 基本概念
- **Dropout**：一种正则化方法，通过在训练过程中随机丢弃部分神经元来减少过拟合。
- **核心思想**：迫使网络学习更鲁棒的特征，不依赖某些特定神经元。

## 2. 工作原理
- **训练阶段**：
  - 以概率 p 将某些神经元的输出置零。
  - 使用“反向缩放 (inverted dropout)”：保留下来的神经元输出除以 (1-p)，保证期望不变。
- **测试阶段**：
  - 不丢弃任何神经元。
  - Dropout 层相当于恒等映射。

## 3. 数学表示
- 对于神经元输出 h：
  \[ h' = \frac{m \cdot h}{1-p} \]
  其中 m ~ Bernoulli(1-p)。

## 4. 优点
- 有效减少过拟合。
- 简单易实现，常作为标准正则化手段。

## 5. 缺点
- 增加训练时间（需要更多迭代收敛）。
- 在小模型或小数据集上可能导致欠拟合。
- 测试时输出不稳定（若未切换到 eval 模式）。

## 6. 常见误区
- **误区1**：Dropout 会影响测试时间 —— 实际上推理时不启用 Dropout，计算量不变。
- **误区2**：Dropout 是加速训练的手段 —— 它可能降低单次训练量，但整体收敛通常更慢。

## 7. 与其他正则化方法对比
- **L2 正则化**：限制权重大小；Dropout 随机屏蔽激活。
- **早停法 (Early Stopping)**：通过停止迭代防止过拟合。
- **数据增强**：在输入层增加多样性；Dropout 在隐藏层增加鲁棒性。

## 8. 实践建议
- 常用 p 范围：0.2 ~ 0.5。
- 常放在全连接层后，卷积层一般较少使用（会影响特征结构）。
- 与 Batch Normalization 结合时注意顺序（一般 BN 在 Dropout 前）。

