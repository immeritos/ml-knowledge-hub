# 梯度裁剪 (Gradient Clipping) — 知识点提纲

## 1. 基本概念
- **梯度裁剪**：在反向传播过程中，对过大的梯度进行限制（截断或缩放），防止梯度爆炸。
- **目标**：保证训练过程稳定，避免参数更新过大。

## 2. 梯度裁剪的常见方式
1. **按元素裁剪 (Element-wise Clipping)**  
   - 若某个梯度分量超过阈值，则将其限制在范围 [-c, c]。
   - 公式：
     \[ g_i \leftarrow clip(g_i, -c, c) \]

2. **按整体范数裁剪 (Norm Clipping, 常用)**  
   - 若梯度向量范数超过阈值，则按比例缩小。
   - 公式：
     \[ g \leftarrow \frac{c}{\|g\|} g \quad (\text{if } \|g\| > c) \]
   - 保留方向不变，仅缩小幅度。

## 3. 应用场景
- **RNN/LSTM 训练**：常遇到梯度爆炸问题，梯度裁剪是标准解决方法。
- **深层神经网络**：避免参数更新过大导致数值不稳定。

## 4. 常见误区
- **误区1**：梯度裁剪用于防止梯度消失。  
  实际上，它主要解决的是 **梯度爆炸**。
- **误区2**：裁剪是为了抬高小梯度。  
  事实上裁剪只限制梯度的上界，不会放大过小的梯度。

## 5. 对比其他方法
- **梯度裁剪 vs 学习率调节**：学习率调整是全局缩放，裁剪是对异常大梯度做限制。
- **梯度裁剪 vs 正则化**：正则化是控制参数大小，裁剪是控制梯度大小。

## 6. 实践建议
- 一般使用 **整体范数裁剪**，更稳定。  
- 常用阈值范围：1 ~ 5。  
- 在框架中可直接调用：
  - PyTorch: `torch.nn.utils.clip_grad_norm_()`  
  - TensorFlow: `tf.clip_by_norm()`  

