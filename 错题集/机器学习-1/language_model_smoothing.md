# 📑 语言模型（MLE / 平滑方法）

## 1. MLE 在语言模型中的应用
- **思想**：根据训练语料统计 n-gram 概率。  
- **公式**：以 bigram 为例  
  ```math
  P_{MLE}(w_i \mid w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
  ```
- **问题**：未出现的 n-gram 概率为 0，导致整句概率归零。  

---

## 2. 平滑的必要性
- **目标**：避免零概率问题。  
- **核心思想**：减少已见事件的概率，把部分概率质量分配给未见事件。  

---

## 3. 常见平滑方法

### （1）加法平滑（Additive / Laplace）
```math
P(w_i \mid w_{i-1}) = \frac{C(w_{i-1}, w_i) + \alpha}{C(w_{i-1}) + \alpha |V|}
```
- α=1 时为 Laplace 平滑，常取 α<1。  

### （2）Good-Turing 平滑
- 思想：利用“出现 r 次事件的数量”来重新分配概率。  
- 调整公式：  
  ```math
  C^*(r) = (r+1) \cdot \frac{N_{r+1}}{N_r}
  ```

### （3）回退法（Back-off）
- 若高阶 n-gram 未出现，则退回低阶模型。  
- 如：bigram 不存在 → 使用 unigram 概率。  

### （4）插值法（Interpolation）
- 同时结合不同阶模型：  
  ```math
  P(w_i \mid w_{i-1}) = \lambda_1 P_{MLE}(w_i \mid w_{i-1}) + \lambda_2 P_{MLE}(w_i)
  ```

### （5）Kneser–Ney 平滑
- 最优方法之一。  
- 在回退时，低阶分布基于“不同上下文中出现的次数”，效果优于普通回退。  

---

## 4. 应用场景
- 早期 n-gram 语言模型（语音识别、机器翻译）。  
- 现代神经语言模型中，仍隐含有类似“平滑”思想（如 label smoothing、Dropout）。  

---

## 5. 一句话总结
- **MLE**：直接基于频率估计，未见事件概率为 0。  
- **平滑**：削减已见概率，给未见事件分配非零概率。  
- **常用方法**：加法、Good-Turing、回退、插值、Kneser–Ney。  
