# 📑 优化方法（梯度下降 / 梯度提升 / SVM 参数调节）

## 1. 梯度下降 (Gradient Descent)
- **目标**：最小化损失函数。  
- **更新公式**：
  ```math
  \theta \leftarrow \theta - \eta \nabla_\theta L(\theta)
  ```
- **直观理解**：在山谷里“往下走”，直到找到最低点。  
- **变体**：批量梯度下降 (BGD)、随机梯度下降 (SGD)、小批量 (Mini-batch)。  
- **应用**：线性回归、逻辑回归、神经网络训练。  

---

## 2. 梯度提升 (Gradient Boosting)
- **目标**：最小化损失函数（与梯度下降相同）。  
- **区别**：优化对象不同。  
  - 梯度下降：在 **参数空间** 中更新参数。  
  - 梯度提升：在 **函数空间** 中迭代学习弱学习器。  
- **流程**：
  1. 初始化预测函数 $F_0(x)$。  
  2. 计算负梯度（即残差）。  
  3. 拟合弱学习器 $h_m(x)$。  
  4. 更新：  
     ```math
     F_m(x) = F_{m-1}(x) + \eta h_m(x)
     ```
- **直观理解**：逐步用新的弱学习器去拟合前一轮的残差。  
- **应用**：GBDT、XGBoost、LightGBM 等。  

---

## 3. SVM 参数调节 ($C$ 与 $\gamma$)
- **$C$ 参数**（惩罚系数）：
  - 控制对训练错误的容忍度。  
  - $C$ 大：错误容忍度低 → 间隔小 → 易过拟合。  
  - $C$ 小：错误容忍度高 → 间隔大 → 泛化好。  

- **$\gamma$ 参数**（核函数参数，RBF 常用）：
  - 控制样本影响范围。  
  - $\gamma$ 大：影响范围小，边界复杂 → 易过拟合。  
  - $\gamma$ 小：影响范围大，边界平滑 → 易欠拟合。  

- **小口诀**：
  - $\gamma$ 大像“显微镜”，只看局部 → 复杂。  
  - $\gamma$ 小像“模糊镜头”，全局趋势 → 平滑。  
  - $C$ 大：对错误零容忍 → 复杂。  
  - $C$ 小：允许错误 → 简单。  

---

## 4. 对比总结

| 方法 | 优化对象 | 特点 | 应用 |
|------|----------|------|------|
| **梯度下降** | 参数空间 | 简单通用，适合连续优化 | 神经网络、回归模型 |
| **梯度提升** | 函数空间 | 集成弱学习器，拟合残差 | GBDT, XGBoost, LightGBM |
| **SVM 参数 $C$/$\gamma$** | 控制模型复杂度 | $C$ 控制错误容忍度，$\gamma$ 控制影响范围 | 分类任务，核方法 |

---

## 5. 一句话总结
- **梯度下降**：参数空间里走 → 最小化损失。  
- **梯度提升**：函数空间里走 → 用弱学习器拟合残差。  
- **SVM 参数 $C$ 与 $\gamma$**：分别调节错误容忍度和影响范围，平衡欠拟合/过拟合。  
