# 📑 集成学习 & 树模型

## 1. 决策树 (Decision Tree)
- **结构**：
  - 内部节点：特征测试条件  
  - 分支：特征取值结果  
  - 叶节点：类别或预测值  
- **分类树**：叶子输出类别，划分依据为信息增益 / 信息增益率 / 基尼指数。  
- **回归树**：叶子输出连续值，划分依据为最小化 MSE。  
- **问题**：容易过拟合，需要剪枝。  

---

## 2. ID3 算法
- **划分准则**：信息增益。  
- **优点**：简单直观。  
- **缺点**：偏好多值特征。  

---

## 3. C4.5 算法
- **划分准则**：信息增益率。  
- **改进**：避免 ID3 偏好多值特征。  
- **启发式策略**：先选信息增益高于平均的特征，再选信息增益率最大的。  
- **优点**：能处理连续特征和缺失值。  

---

## 4. 随机森林 (Random Forest)
- **思想**：Bagging + 随机特征子集，集成多棵决策树。  
- **优点**：鲁棒性强，能处理高维特征，不易过拟合。  
- **缺点**：
  - 在大规模稀疏特征下，训练/预测变慢。  
  - 稀疏长尾特征难以捕捉，特征重要性估计偏差。  

---

## 5. GBDT (Gradient Boosting Decision Tree)
- **思想**：Boosting 思路，每一棵树拟合前一轮的残差。  
- **优化目标**：最小化损失函数。  
- **优点**：拟合能力强，泛化性好。  
- **缺点**：训练较慢，参数较多。  

---

## 6. XGBoost
- **改进点**：
  - 二阶导数优化，近似直方图算法。  
  - 正则化项，控制模型复杂度。  
  - 支持分布式训练。  
- **分裂策略**：level-wise（逐层分裂）。  
- **缺点**：内存占用大。  

---

## 7. LightGBM
- **改进点**：
  - 基于直方图的高效分裂算法（更快、更省内存）。  
  - 支持类别特征处理。  
  - 高效的并行化。  
- **分裂策略**：leaf-wise（优先分裂增益最大的叶子）。  
- **优点**：速度快，内存占用低，适合大数据。  
- **缺点**：leaf-wise 易过拟合。  

---

## 8. 对比总结

| 模型 | 核心思想 | 优点 | 缺点 | 适用场景 |
|------|----------|------|------|----------|
| 决策树 | 特征划分构建树 | 可解释性强 | 容易过拟合 | 小数据集，可解释需求强 |
| 随机森林 | Bagging + 随机特征 | 鲁棒，不易过拟合 | 稀疏特征下慢 | 高维特征，非稀疏数据 |
| GBDT | Boosting + 残差拟合 | 泛化性好 | 训练慢 | 中小数据集，精度优先 |
| XGBoost | 改进版 GBDT | 高效，分布式，正则化 | 内存占用大 | 大规模数据，工业界常用 |
| LightGBM | 基于直方图的高效 GBDT | 更快，更省内存 | 易过拟合 | 超大规模数据，分布式场景 |

---

## 9. 一句话总结
- **决策树**：基础模型，容易过拟合。  
- **随机森林**：Bagging 思路，稳健但稀疏数据下慢。  
- **GBDT**：Boosting 思路，拟合能力强但慢。  
- **XGBoost**：GBDT 工程优化版，工业级应用广泛。  
- **LightGBM**：更快更省内存，适合超大数据。  
